{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymed import PubMed\n",
    "import collections\n",
    "import time\n",
    "import openai\n",
    "import local_settings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "openai.api_key = local_settings.OAI_API_KEY\n",
    "\n",
    "pubmed = PubMed(tool=\"aiLITsearch\", email=local_settings.EMAIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_author_name(author):\n",
    "    if author['firstname'] and author['lastname'] and author['initials']:\n",
    "        return f\"{author['firstname']} {author['initials']} {author['lastname']}\"\n",
    "    elif author['firstname'] and author['lastname']:\n",
    "        return f\"{author['firstname']} {author['lastname']}\"\n",
    "    elif author['lastname']:\n",
    "        return f\"{author['lastname']}\"\n",
    "    elif author['initials']:\n",
    "        return f\"{author['initials']}\"\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "\n",
    "def pubmed_query(query, num_results=500):\n",
    "    results = pubmed.query(query, max_results=num_results)\n",
    "    print(f'Got results for {query}... unpacking..!')\n",
    "    pubs = []\n",
    "\n",
    "    for result in results:\n",
    "        try:\n",
    "            authors = [\n",
    "                {\n",
    "                    'name': _parse_author_name(author),\n",
    "                    'affiliation': author['affiliation']\n",
    "                } for author in result.authors\n",
    "            ]\n",
    "            if results.keywords:\n",
    "                keywords = set([k.lower() for k in result.keywords if k and k.strip()])\n",
    "            rel_data = {\n",
    "                'abstract': result.abstract,\n",
    "                'title': result.title,\n",
    "                'keywords': keywords,\n",
    "                'journal': result.journal,\n",
    "                'publication_date': result.publication_date,\n",
    "                'authors': authors,\n",
    "                'doi': result.doi,\n",
    "                'pubmed_id': result.pubmed_id,\n",
    "                'results': result.results,\n",
    "                'conclusions': result.conclusions,\n",
    "            }\n",
    "            pubs.append(rel_data)\n",
    "        except Exception as e:\n",
    "            print(f'Error unpacking {result.title}: {e}')\n",
    "            continue\n",
    "    \n",
    "    print(f'Done unpacking. Returning {len(pubs)} results')\n",
    "    return pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_keywords(papers, min_count=2):\n",
    "    keywords = []\n",
    "    for paper in papers:\n",
    "        keywords.extend(paper['keywords'])\n",
    "\n",
    "    keywords = collections.Counter(keywords)\n",
    "    keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "    keywords = [k[0] for k in keywords if k[1] > min_count]\n",
    "    print(f'Found {len(keywords)} keywords when min_count={min_count}')\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brainstorm_keywords(paper, all_keywords, model='gpt-4'):\n",
    "    keywords_str = '\\n'.join(all_keywords)\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "You are a researcher AI designed to help humans categorize research papers. \n",
    "\n",
    "You are given a research paper and asked to brainstorm keywords that describe the paper.\n",
    "\n",
    "You are also given a list of keywords that have been used to describe similar papers.\n",
    "\n",
    "Please select the keywords from this list that best describe the provided abstract. \n",
    "\"\"\"\n",
    "\n",
    "    KEYWORD_LIST_PROMPT = f\"\"\"\n",
    "These are the keywords that have been used to describe similar papers. \n",
    "\n",
    "Only use these keywords, any other output will be filtered by post-processing.\n",
    "\n",
    "## Keywords:\n",
    "{keywords_str}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    PAPER_PROMPT = f\"\"\"\n",
    "This is the paper you are labeling. The keywords provided are human generated.\n",
    "\n",
    "## Paper:\n",
    "TITLE: {paper['title']}\n",
    "ABSTRACT: {paper['abstract']}\n",
    "KEYWORDS: [{', '.join(paper['keywords'])}]\n",
    "\"\"\"\n",
    "    prompt = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"system\", \"content\": PAPER_PROMPT},\n",
    "            {\"role\": \"system\", \"content\": KEYWORD_LIST_PROMPT},\n",
    "            {\"role\": \"assistant\", \"content\": \"Keywords:\"}\n",
    "        ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=prompt,\n",
    "        max_tokens=150,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    # Extract generated keywords\n",
    "    keywords = response.choices[0]\n",
    "    usage = response.usage\n",
    "    keywords = keywords[\"message\"][\"content\"].split('\\n')[0].split(', ')\n",
    "    \n",
    "    # Remove leading and trailing whitespaces\n",
    "    keywords = [keyword.strip() for keyword in keywords if keyword in all_keywords]\n",
    "    return keywords, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_test(model, papers, kw_min=2, num_papers=10):\n",
    "    if model == 'gpt-4':\n",
    "        PRICE_PER_1K_TOKENS = 0.03\n",
    "        print('Using GPT-4')\n",
    "    elif model == 'gpt-3.5-turbo':\n",
    "        PRICE_PER_1K_TOKENS = 0.003\n",
    "        print('Using GPT-3.5 Turbo')\n",
    "    \n",
    "    total_usage = {\n",
    "        'tokens': 0\n",
    "    }\n",
    "\n",
    "    keywords_kw_min = get_all_keywords(papers, min_count=kw_min)\n",
    "\n",
    "    for paper_idx, paper in enumerate(papers[:num_papers]):\n",
    "        paper['bot_keywords'] = []\n",
    "        try:\n",
    "            bot_keywords, usage = brainstorm_keywords(paper, keywords_kw_min, model=model)\n",
    "        except Exception as e:\n",
    "            time.sleep(10)\n",
    "            print(f'Retrying paper {paper_idx} ({paper[\"title\"]})')\n",
    "            bot_keywords, usage = brainstorm_keywords(paper, model=model)\n",
    "        bot_keywords = [keyword for keyword in bot_keywords if keyword not in paper['keywords']]\n",
    "        paper['bot_keywords'] += bot_keywords\n",
    "        print('###############################################')\n",
    "        print(f'Added {len(bot_keywords)} keywords to paper {paper_idx} ({paper[\"title\"]})')\n",
    "        print(f'Human keywords: {paper[\"keywords\"]}')\n",
    "        print(f'Bot keywords: {paper[\"bot_keywords\"]}')\n",
    "\n",
    "        total_usage['tokens'] += usage['total_tokens']\n",
    "        \n",
    "        paper['bot_keywords'] = list(set(paper['bot_keywords']))\n",
    "        print(f'Used {total_usage[\"tokens\"]} total tokens (${(PRICE_PER_1K_TOKENS * total_usage[\"tokens\"]) / 1000:.2f})')\n",
    "\n",
    "\n",
    "\n",
    "def _run_test_2(papers, kw_min=2, num_papers=10):\n",
    "    \n",
    "    total_usage = {\n",
    "        'usd': 0\n",
    "    }\n",
    "\n",
    "    model_prices = {\n",
    "        'gpt-4': 0.03,\n",
    "        'gpt-3.5-turbo': 0.003,\n",
    "    }\n",
    "\n",
    "    keywords_kw_min = get_all_keywords(papers, min_count=kw_min)\n",
    "    papers_stats = {}\n",
    "    for paper_idx, paper in enumerate(papers[:num_papers]):\n",
    "        model_keywords = {}\n",
    "        for model in ['gpt-4', 'gpt-3.5-turbo']:\n",
    "            try:\n",
    "                bot_keywords, usage = brainstorm_keywords(paper, all_keywords=keywords_kw_min, model=model)\n",
    "            except Exception as e:\n",
    "                time.sleep(10)\n",
    "                print(f'Retrying paper {paper_idx} ({paper[\"title\"]})')\n",
    "                bot_keywords, usage = brainstorm_keywords(paper, all_keywords=keywords_kw_min,  model=model)\n",
    "\n",
    "            total_usage['usd'] += (usage['total_tokens'] / 1000) * model_prices[model]\n",
    "            bot_keywords = [keyword for keyword in bot_keywords if keyword not in paper['keywords']]\n",
    "            model_keywords[model] = {\n",
    "                'n': len(bot_keywords),\n",
    "                'keywords': bot_keywords,\n",
    "            }\n",
    "\n",
    "                \n",
    "        papers_stats[paper_idx] = {\n",
    "            'title': paper['title'],\n",
    "            'abstract': paper['abstract'],\n",
    "            'human_keywords': paper['keywords'],\n",
    "            'n_human_keywords': len(paper['keywords']),\n",
    "            'gpt-4_n_keywords': model_keywords['gpt-4']['n'],\n",
    "            'gpt-3.5_n_keywords': model_keywords['gpt-3.5-turbo']['n'],\n",
    "            'gpt-4_keywords': model_keywords['gpt-4']['keywords'],\n",
    "            'gpt-3.5_keywords': model_keywords['gpt-3.5-turbo']['keywords'],\n",
    "        }\n",
    "\n",
    "        print(f'Proccessed paper {paper_idx}')\n",
    "\n",
    "    return papers_stats, total_usage['usd']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got results for microbiome... unpacking..!\n",
      "Error unpacking Distribution patterns and driving mechanism of soil protozoan community at the different depths of : 'NoneType' object has no attribute 'lower'\n",
      "Error unpacking Drugs and Lactation Database (LactMed®): 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking Drugs and Lactation Database (LactMed®): 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking Drugs and Lactation Database (LactMed®): 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking Drugs and Lactation Database (LactMed®): 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking Drugs and Lactation Database (LactMed®): 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking Drugs and Lactation Database (LactMed®): 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking Total Transit Time and Probiotic Persistence in Healthy Adults: A Pilot Study.: 'NoneType' object has no attribute 'lower'\n",
      "Done unpacking. Returning 4992 results\n",
      "Got results for machine learning... unpacking..!\n",
      "Error unpacking Extraction of tree crown parameters of high-density pure : 'NoneType' object has no attribute 'lower'\n",
      "Error unpacking Total Transit Time and Probiotic Persistence in Healthy Adults: A Pilot Study.: 'NoneType' object has no attribute 'lower'\n",
      "Done unpacking. Returning 4998 results\n",
      "Got results for marijuana... unpacking..!\n",
      "Error unpacking Drugs and Lactation Database (LactMed®): 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking Drugs and Lactation Database (LactMed®): 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking LiverTox: Clinical and Research Information on Drug-Induced Liver Injury: 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking LiverTox: Clinical and Research Information on Drug-Induced Liver Injury: 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking Healthcare Cost and Utilization Project (HCUP) Statistical Briefs: 'affiliation'\n",
      "Error unpacking Drugs and Lactation Database (LactMed®): 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking PDQ Cancer Information Summaries: 'affiliation'\n",
      "Error unpacking Mother To Baby | Fact Sheets: 'PubMedBookArticle' object has no attribute 'keywords'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking Living Systematic Review on Cannabis and Other Plant-Based Treatments for Chronic Pain: 2022 Update: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking PDQ Cancer Information Summaries: 'affiliation'\n",
      "Error unpacking RTI Press Research Brief: 'affiliation'\n",
      "Error unpacking Living Systematic Review on Cannabis and Other Plant-Based Treatments for Chronic Pain: Interim Progress and Surveillance Reports: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking Living Systematic Review on Cannabis and Other Plant-Based Treatments for Chronic Pain: Interim Progress and Surveillance Reports: 'affiliation'\n",
      "Error unpacking StatPearls: 'affiliation'\n",
      "Error unpacking Healthcare Cost and Utilization Project (HCUP) Statistical Briefs: 'affiliation'\n",
      "Error unpacking Therapeutic Use of Medical Cannabis in New York State: 'affiliation'\n",
      "Done unpacking. Returning 4966 results\n"
     ]
    }
   ],
   "source": [
    "num_pubmed_results = 5000\n",
    "\n",
    "microbiome_papers = pubmed_query('microbiome', num_results=num_pubmed_results)\n",
    "ml_papers = pubmed_query('machine learning', num_results=num_pubmed_results)\n",
    "marijuana_papers = pubmed_query('marijuana', num_results=num_pubmed_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPT-4\n",
      "###############################################\n",
      "Added 4 keywords to paper 0 (Identification of key degraders for controlling toxicity risks of disguised toxic pollutants with division of labor mechanisms in activated sludge microbiomes: Using nonylphenol ethoxylate as an example.)\n",
      "Human keywords: {'division of labor', 'sphingobium', 'disguised toxic pollutants', 'nonylphenol ethoxylate', 'pseudomonas'}\n",
      "Bot keywords: ['microbiome', 'bacteria', 'bioremediation', 'microbial community']\n",
      "Used 1012 total tokens ($0.03)\n",
      "###############################################\n",
      "Added 19 keywords to paper 1 (Microbes, metabolites and muscle: Is the gut-muscle axis a plausible therapeutic target in Duchenne muscular dystrophy?)\n",
      "Human keywords: {'duchenne muscular dystrophy', 'metabolic signalling', 'gut-muscle axis', 'gut microbial therapies'}\n",
      "Bot keywords: ['gut microbiota', 'microbiome', 'inflammation', 'metabolism', 'bacteria', 'metabolites', 'probiotics', 'dysbiosis', 'short-chain fatty acids', 'intestinal flora', 'prebiotics', 'gut-brain axis', 'cytokines', 'bile acids', 'fecal microbiota transplantation', 'immune system', 'gene expression', 'microbiota-gut-brain axis', 'treatment']\n",
      "Used 2112 total tokens ($0.06)\n",
      "###############################################\n",
      "Added 7 keywords to paper 2 (The Microbiome in Advanced Melanoma: Where Are We Now?)\n",
      "Human keywords: {'immunotherapy', 'melanoma', 'cytotoxic t-lymphocyte antigen-4 (ctla-4)', 'gut microbiome', 'programmed death-1 (pd-1)'}\n",
      "Bot keywords: ['gut microbiota', 'microbiome', 'immunity', 'cancer', 'intestinal flora', 'tumor microenvironment', 'fecal microbiota transplantation']\n",
      "Used 2958 total tokens ($0.09)\n",
      "###############################################\n",
      "Added 9 keywords to paper 3 (Letter to the Editor mucosal gut microbiota in primary sclerosing cholangitis before and after liver transplantation: Are there other challenges?)\n",
      "Human keywords: set()\n",
      "Bot keywords: ['gut microbiota', 'microbiome', 'inflammation', 'inflammatory bowel disease', 'dysbiosis', 'intestinal flora', 'intestinal microbiota', 'bile acids', 'ulcerative colitis']\n",
      "Used 3587 total tokens ($0.11)\n",
      "###############################################\n",
      "Added 10 keywords to paper 4 (rTMS ameliorates depressive-like behaviors and regulates the gut microbiome and medium- and long-chain fatty acids in mice exposed to chronic unpredictable mild stress.)\n",
      "Human keywords: {'gut microbiota', 'rtms', 'cums', 'medium- and long-chain fatty acids'}\n",
      "Bot keywords: ['microbiome', 'metabolism', 'bacteria', 'gut-brain axis', 'inflammation', 'neuroinflammation', 'anxiety', 'treatment', 'cognitive function', 'microbiota-gut-brain axis']\n",
      "Used 4516 total tokens ($0.14)\n",
      "###############################################\n",
      "Added 5 keywords to paper 5 (Microbial inoculums improve growth and health of Heteropneustes fossilis via biofloc-driven aquaculture.)\n",
      "Human keywords: {'heteropneustes fossilis', 'growth performance', 'biofloc technology', 'immunity', 'microbial inoculums', 'aquaculture', 'water quality'}\n",
      "Bot keywords: ['gut microbiota', 'microbiome', 'bacteria', 'probiotics', 'bioremediation']\n",
      "Used 5687 total tokens ($0.17)\n",
      "###############################################\n",
      "Added 7 keywords to paper 6 (Effects of Si-Miao-Yong-An decoction on myocardial I/R rats by regulating gut microbiota to inhibit LPS-induced TLR4/NF-κB signaling pathway.)\n",
      "Human keywords: {'intestinal permeability', 'gut microbiota', 'si-miao-yong-an', 'ischemia/reperfusion', 'tlr4/nf-κb'}\n",
      "Bot keywords: ['inflammation', 'intestinal flora', 'intestinal microbiota', 'biomarkers', 'cytokines', 'cardiovascular disease', 'intestine']\n",
      "Used 6820 total tokens ($0.20)\n",
      "###############################################\n",
      "Added 6 keywords to paper 7 (Soil microbiomes must be explicitly included in One Health policy.)\n",
      "Human keywords: set()\n",
      "Bot keywords: ['microbiome', 'microbiota', 'bacteria', 'diversity', 'rhizosphere', 'microbial community']\n",
      "Used 7415 total tokens ($0.22)\n",
      "###############################################\n",
      "Added 5 keywords to paper 8 (Ultra-high performance liquid chromatography high-resolution mass spectrometry for metabolomic analysis of dental calculus from Duke Alessandro Farnese and Maria D'Aviz.)\n",
      "Human keywords: set()\n",
      "Bot keywords: ['metabolomics', 'metabolome', 'biofilm', 'bacteria', 'metabolites']\n",
      "Used 8214 total tokens ($0.25)\n",
      "###############################################\n",
      "Added 6 keywords to paper 9 (Gaming bacterial metabolism.)\n",
      "Human keywords: set()\n",
      "Bot keywords: ['metabolism', 'bacteria', 'gene expression', 'escherichia coli', 'growth performance', 'microbiota']\n",
      "Used 8796 total tokens ($0.26)\n"
     ]
    }
   ],
   "source": [
    "_run_test('gpt-4', microbiome_papers, num_papers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPT-3.5 Turbo\n",
      "###############################################\n",
      "Added 0 keywords to paper 0 (Identification of key degraders for controlling toxicity risks of disguised toxic pollutants with division of labor mechanisms in activated sludge microbiomes: Using nonylphenol ethoxylate as an example.)\n",
      "Human keywords: {'division of labor', 'sphingobium', 'disguised toxic pollutants', 'nonylphenol ethoxylate', 'pseudomonas'}\n",
      "Bot keywords: []\n",
      "Used 1044 total tokens ($0.00)\n",
      "###############################################\n",
      "Added 1 keywords to paper 1 (Microbes, metabolites and muscle: Is the gut-muscle axis a plausible therapeutic target in Duchenne muscular dystrophy?)\n",
      "Human keywords: {'duchenne muscular dystrophy', 'metabolic signalling', 'gut-muscle axis', 'gut microbial therapies'}\n",
      "Bot keywords: ['short-chain fatty acids']\n",
      "Used 2141 total tokens ($0.01)\n",
      "###############################################\n",
      "Added 1 keywords to paper 2 (The Microbiome in Advanced Melanoma: Where Are We Now?)\n",
      "Human keywords: {'immunotherapy', 'melanoma', 'cytotoxic t-lymphocyte antigen-4 (ctla-4)', 'gut microbiome', 'programmed death-1 (pd-1)'}\n",
      "Bot keywords: ['probiotics']\n",
      "Used 3005 total tokens ($0.01)\n",
      "###############################################\n",
      "Added 0 keywords to paper 3 (Letter to the Editor mucosal gut microbiota in primary sclerosing cholangitis before and after liver transplantation: Are there other challenges?)\n",
      "Human keywords: set()\n",
      "Bot keywords: []\n",
      "Used 3614 total tokens ($0.01)\n",
      "###############################################\n",
      "Added 0 keywords to paper 4 (rTMS ameliorates depressive-like behaviors and regulates the gut microbiome and medium- and long-chain fatty acids in mice exposed to chronic unpredictable mild stress.)\n",
      "Human keywords: {'gut microbiota', 'rtms', 'cums', 'medium- and long-chain fatty acids'}\n",
      "Bot keywords: []\n",
      "Used 4517 total tokens ($0.01)\n",
      "###############################################\n",
      "Added 2 keywords to paper 5 (Microbial inoculums improve growth and health of Heteropneustes fossilis via biofloc-driven aquaculture.)\n",
      "Human keywords: {'heteropneustes fossilis', 'growth performance', 'biofloc technology', 'immunity', 'microbial inoculums', 'aquaculture', 'water quality'}\n",
      "Bot keywords: ['probiotics', 'bioremediation']\n",
      "Used 5773 total tokens ($0.02)\n",
      "###############################################\n",
      "Added 1 keywords to paper 6 (Effects of Si-Miao-Yong-An decoction on myocardial I/R rats by regulating gut microbiota to inhibit LPS-induced TLR4/NF-κB signaling pathway.)\n",
      "Human keywords: {'intestinal permeability', 'gut microbiota', 'si-miao-yong-an', 'ischemia/reperfusion', 'tlr4/nf-κb'}\n",
      "Bot keywords: ['inflammation']\n",
      "Used 7013 total tokens ($0.02)\n",
      "###############################################\n",
      "Added 0 keywords to paper 7 (Soil microbiomes must be explicitly included in One Health policy.)\n",
      "Human keywords: set()\n",
      "Bot keywords: []\n",
      "Used 7596 total tokens ($0.02)\n",
      "###############################################\n",
      "Added 1 keywords to paper 8 (Ultra-high performance liquid chromatography high-resolution mass spectrometry for metabolomic analysis of dental calculus from Duke Alessandro Farnese and Maria D'Aviz.)\n",
      "Human keywords: set()\n",
      "Bot keywords: ['metabolomics']\n",
      "Used 8435 total tokens ($0.03)\n",
      "###############################################\n",
      "Added 2 keywords to paper 9 (Gaming bacterial metabolism.)\n",
      "Human keywords: set()\n",
      "Bot keywords: ['bacteria', 'metabolism']\n",
      "Used 9007 total tokens ($0.03)\n"
     ]
    }
   ],
   "source": [
    "_run_test('gpt-3.5-turbo', microbiome_papers, num_papers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPT-4\n",
      "###############################################\n",
      "Added 10 keywords to paper 0 (A Bayesian approach to predictive uncertainty in chemotherapy patients at risk of acute care utilization.)\n",
      "Human keywords: {'acute care utilization', 'bayesian logistic lasso regression', 'chemotherapy', 'predictive uncertainty'}\n",
      "Bot keywords: ['cancer', 'biomarkers', 'colorectal cancer', 'breast cancer', 'tumor microenvironment', 'inflammation', 'immune system', 'immunotherapy', 'treatment', 'diagnosis']\n",
      "Used 979 total tokens ($0.03)\n",
      "###############################################\n",
      "Added 5 keywords to paper 1 (Identification of necroptosis-related long non-coding RNAs prognostic signature and the crucial lncRNA in bladder cancer.)\n",
      "Human keywords: {'prognosis', 'mafg-dt', 'lncrnas', 'necroptosis', 'bladder cancer'}\n",
      "Bot keywords: ['cancer', 'biomarkers', 'gene expression', 'tumor microenvironment', 'diagnosis']\n",
      "Used 2098 total tokens ($0.06)\n",
      "###############################################\n",
      "Added 0 keywords to paper 2 (Single-trial extraction of event-related potentials (ERPs) and classification of visual stimuli by ensemble use of discrete wavelet transform with Huffman coding and machine learning techniques.)\n",
      "Human keywords: {'discrete wavelet transform', 'huffman coding', 'visual object detection', 'single trials analysis (erps)', 'machine learning classifiers'}\n",
      "Bot keywords: []\n",
      "Used 3150 total tokens ($0.09)\n",
      "###############################################\n",
      "Added 7 keywords to paper 3 (Prediction of treatment response to antipsychotic drugs for precision medicine approach to schizophrenia: randomized trials and multiomics analysis.)\n",
      "Human keywords: {'epigenetics', 'antipsychotic drug', 'treatment response', 'genetics', 'schizophrenia', 'prediction model'}\n",
      "Bot keywords: ['biomarkers', 'inflammation', 'gene expression', 'treatment', 'diagnosis', 'immune system', 'neuroinflammation']\n",
      "Used 4400 total tokens ($0.13)\n",
      "###############################################\n",
      "Added 0 keywords to paper 4 (Merging bioactivity predictions from cell morphology and chemical fingerprint models using similarity to training data.)\n",
      "Human keywords: {'applicability domain', 'bioactivity', 'toxicity', 'structure', 'cell painting', 'machine learning'}\n",
      "Bot keywords: []\n",
      "Used 5253 total tokens ($0.16)\n",
      "###############################################\n",
      "Added 0 keywords to paper 5 (Machine diagnosis of chronic obstructive pulmonary disease using a novel fast-response capnometer.)\n",
      "Human keywords: {'chronic obstructive\\xa0pulmonary disease', 'machine learning', 'diagnosis'}\n",
      "Bot keywords: []\n",
      "Used 6092 total tokens ($0.18)\n",
      "###############################################\n",
      "Added 1 keywords to paper 6 (Open-source environmental data as an alternative to snail surveys to assess schistosomiasis risk in areas approaching elimination.)\n",
      "Human keywords: {'oncomelania hupensis', 'prevention and control', 'geographic information systems', 'snails', 'schistosomiasis', 'machine learning', 'china', 'infectious disease surveillance', 'remote sensing technology'}\n",
      "Bot keywords: ['infection']\n",
      "Used 7161 total tokens ($0.21)\n",
      "###############################################\n",
      "Added 5 keywords to paper 7 (Radiomics for therapy-specific head and neck squamous cell carcinoma survival prognostication (part I).)\n",
      "Human keywords: {'survival prediction', 'radiomics', 'artificial intelligence', 'machine learning', 'medical imaging'}\n",
      "Bot keywords: ['cancer', 'biomarkers', 'tumor microenvironment', 'treatment', 'diagnosis']\n",
      "Used 8205 total tokens ($0.25)\n",
      "###############################################\n",
      "Added 3 keywords to paper 8 (Differential diagnosis of hepatocellular carcinoma and intrahepatic cholangiocarcinoma based on spatial and channel attention mechanisms.)\n",
      "Human keywords: {'deep learning', 'radiomics', 'channel and spatial attention mechanisms', 'intrahepatic cholangiocarcinoma', 'differential diagnosis', 'hepatocellular carcinoma'}\n",
      "Bot keywords: ['cancer', 'diagnosis', 'non-alcoholic fatty liver disease']\n",
      "Used 9299 total tokens ($0.28)\n",
      "###############################################\n",
      "Added 5 keywords to paper 9 (Development and Validation of Multi-Omics Thymoma Risk Classification Model Based on Transfer Learning.)\n",
      "Human keywords: {'ct images', 'transfer learning', 'thymoma', 'machine learning', 'risk stratification'}\n",
      "Bot keywords: ['cancer', 'biomarkers', 'tumor microenvironment', 'diagnosis', 'treatment']\n",
      "Used 10310 total tokens ($0.31)\n"
     ]
    }
   ],
   "source": [
    "_run_test('gpt-4', ml_papers, num_papers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPT-3.5 Turbo\n",
      "Found 37 keywords when min_count=2\n",
      "###############################################\n",
      "Added 5 keywords to paper 0 (Deep Learning Model Based on Dual-Modal Ultrasound and Molecular Data for Predicting Response to Neoadjuvant Chemotherapy in Breast Cancer.)\n",
      "Human keywords: {'deep learning', 'chemotherapy', 'elasticity imaging techniques', 'breast neoplasms'}\n",
      "Bot keywords: ['radiomics', 'support vector machine', 'convolutional neural network', 'prediction model', 'biomarkers']\n",
      "Used 730 total tokens ($0.00)\n",
      "###############################################\n",
      "Added 0 keywords to paper 1 (Speech comprehension across time, space, frequency, and age: MEG-MVPA classification of intertrial phase coherence.)\n",
      "Human keywords: set()\n",
      "Bot keywords: []\n",
      "Used 1307 total tokens ($0.00)\n",
      "###############################################\n",
      "Added 2 keywords to paper 2 (Lung cancer lesion detection in histopathology images using graph-based sparse PCA network.)\n",
      "Human keywords: {'graph-based sparse pca', 'computational imaging', 'cancer lesion detection', 'image analysis', 'machine learning'}\n",
      "Bot keywords: ['lung cancer', 'support vector machine']\n",
      "Used 1956 total tokens ($0.01)\n",
      "###############################################\n",
      "Added 2 keywords to paper 3 (Machine Learning Analysis of Postlaparoscopy Hernias and \"I'm Leaving You to Close\" Strategy.)\n",
      "Human keywords: {'laparoscopy', 'incisional hernia', 'cholecystectomy', 'residency education', 'postoperative complication'}\n",
      "Bot keywords: ['machine learning', 'random forest']\n",
      "Used 2885 total tokens ($0.01)\n",
      "###############################################\n",
      "Added 0 keywords to paper 4 (Role of transition metal d-orbitals in single-atom catalysts for nitric oxide electroreduction to ammonia.)\n",
      "Human keywords: {'no removal', 'phosphorus carbide', 'computational screening', 'machine learning', 'ammonia synthesis'}\n",
      "Bot keywords: []\n",
      "Used 3253 total tokens ($0.01)\n",
      "###############################################\n",
      "Added 0 keywords to paper 5 (Framework for data-driven polymer characterization from infrared spectra.)\n",
      "Human keywords: {'micropolymers', 'artificial intelligence', 'infrared spectroscopy', 'polymer characterization', 'machine learning', 'algorithms'}\n",
      "Bot keywords: []\n",
      "Used 3879 total tokens ($0.01)\n",
      "###############################################\n",
      "Added 1 keywords to paper 6 (A Bayesian approach to predictive uncertainty in chemotherapy patients at risk of acute care utilization.)\n",
      "Human keywords: {'acute care utilization', 'bayesian logistic lasso regression', 'chemotherapy', 'predictive uncertainty'}\n",
      "Bot keywords: ['risk assessment']\n",
      "Used 4578 total tokens ($0.01)\n",
      "###############################################\n",
      "Added 3 keywords to paper 7 (Identification of necroptosis-related long non-coding RNAs prognostic signature and the crucial lncRNA in bladder cancer.)\n",
      "Human keywords: {'prognosis', 'mafg-dt', 'lncrnas', 'necroptosis', 'bladder cancer'}\n",
      "Bot keywords: ['machine learning', 'random forest', 'support vector machine']\n",
      "Used 5502 total tokens ($0.02)\n",
      "###############################################\n",
      "Added 0 keywords to paper 8 (Single-trial extraction of event-related potentials (ERPs) and classification of visual stimuli by ensemble use of discrete wavelet transform with Huffman coding and machine learning techniques.)\n",
      "Human keywords: {'discrete wavelet transform', 'huffman coding', 'visual object detection', 'single trials analysis (erps)', 'machine learning classifiers'}\n",
      "Bot keywords: []\n",
      "Used 6309 total tokens ($0.02)\n",
      "###############################################\n",
      "Added 1 keywords to paper 9 (Prediction of treatment response to antipsychotic drugs for precision medicine approach to schizophrenia: randomized trials and multiomics analysis.)\n",
      "Human keywords: {'epigenetics', 'antipsychotic drug', 'treatment response', 'genetics', 'schizophrenia', 'prediction model'}\n",
      "Bot keywords: ['precision medicine']\n",
      "Used 7293 total tokens ($0.02)\n"
     ]
    }
   ],
   "source": [
    "_run_test('gpt-3.5-turbo', ml_papers, num_papers=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 127 keywords when min_count=10\n",
      "Retrying paper 0 (Deep Learning Model Based on Dual-Modal Ultrasound and Molecular Data for Predicting Response to Neoadjuvant Chemotherapy in Breast Cancer.)\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "<empty message>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 54\u001b[0m, in \u001b[0;36m_run_test_2\u001b[0;34m(papers, kw_min, num_papers)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     bot_keywords, usage \u001b[39m=\u001b[39m brainstorm_keywords(paper, all_keywords\u001b[39m=\u001b[39;49mkeywords_kw_min, model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[72], line 38\u001b[0m, in \u001b[0;36mbrainstorm_keywords\u001b[0;34m(paper, all_keywords, model)\u001b[0m\n\u001b[1;32m     32\u001b[0m prompt \u001b[39m=\u001b[39m [\n\u001b[1;32m     33\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: SYSTEM_PROMPT},\n\u001b[1;32m     34\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: PAPER_PROMPT},\n\u001b[1;32m     35\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: KEYWORD_LIST_PROMPT},\n\u001b[1;32m     36\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mKeywords:\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     37\u001b[0m     ]\n\u001b[0;32m---> 38\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     39\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     40\u001b[0m     messages\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     41\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m,\n\u001b[1;32m     42\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[39m# Extract generated keywords\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/matt/projects/ai-lit-search/venv/lib/python3.8/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/matt/projects/ai-lit-search/venv/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    138\u001b[0m (\n\u001b[1;32m    139\u001b[0m     deployment_id,\n\u001b[1;32m    140\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m )\n\u001b[0;32m--> 153\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m     url,\n\u001b[1;32m    156\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/matt/projects/ai-lit-search/venv/lib/python3.8/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    220\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m     method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m )\n\u001b[0;32m--> 230\u001b[0m resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Documents/matt/projects/ai-lit-search/venv/lib/python3.8/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    629\u001b[0m         ),\n\u001b[1;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/matt/projects/ai-lit-search/venv/lib/python3.8/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: <empty message>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m### Minimum number of times keyword must appear to be included\u001b[39;00m\n\u001b[1;32m      4\u001b[0m kw_min \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m----> 6\u001b[0m ml_stats, ml_usage \u001b[39m=\u001b[39m _run_test_2(ml_papers, num_papers\u001b[39m=\u001b[39;49mnum_papers, kw_min\u001b[39m=\u001b[39;49mkw_min)\n\u001b[1;32m      7\u001b[0m microbiome_stats, mb_usage \u001b[39m=\u001b[39m _run_test_2(microbiome_papers, num_papers\u001b[39m=\u001b[39mnum_papers, kw_min\u001b[39m=\u001b[39mkw_min)\n\u001b[1;32m      8\u001b[0m marijuana_stats, mj_usage \u001b[39m=\u001b[39m _run_test_2(marijuana_papers, num_papers\u001b[39m=\u001b[39mnum_papers, kw_min\u001b[39m=\u001b[39mkw_min)\n",
      "Cell \u001b[0;32mIn[112], line 58\u001b[0m, in \u001b[0;36m_run_test_2\u001b[0;34m(papers, kw_min, num_papers)\u001b[0m\n\u001b[1;32m     56\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m10\u001b[39m)\n\u001b[1;32m     57\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRetrying paper \u001b[39m\u001b[39m{\u001b[39;00mpaper_idx\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mpaper[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m     bot_keywords, usage \u001b[39m=\u001b[39m brainstorm_keywords(paper, all_keywords\u001b[39m=\u001b[39;49mkeywords_kw_min,  model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m     60\u001b[0m total_usage[\u001b[39m'\u001b[39m\u001b[39musd\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (usage[\u001b[39m'\u001b[39m\u001b[39mtotal_tokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m \u001b[39m1000\u001b[39m) \u001b[39m*\u001b[39m model_prices[model]\n\u001b[1;32m     61\u001b[0m bot_keywords \u001b[39m=\u001b[39m [keyword \u001b[39mfor\u001b[39;00m keyword \u001b[39min\u001b[39;00m bot_keywords \u001b[39mif\u001b[39;00m keyword \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m paper[\u001b[39m'\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[72], line 38\u001b[0m, in \u001b[0;36mbrainstorm_keywords\u001b[0;34m(paper, all_keywords, model)\u001b[0m\n\u001b[1;32m     24\u001b[0m     PAPER_PROMPT \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39mThis is the paper you are labeling. The keywords provided are human generated.\u001b[39m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mKEYWORDS: [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(paper[\u001b[39m'\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     prompt \u001b[39m=\u001b[39m [\n\u001b[1;32m     33\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: SYSTEM_PROMPT},\n\u001b[1;32m     34\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: PAPER_PROMPT},\n\u001b[1;32m     35\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: KEYWORD_LIST_PROMPT},\n\u001b[1;32m     36\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mKeywords:\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     37\u001b[0m         ]\n\u001b[0;32m---> 38\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     39\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     40\u001b[0m         messages\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     41\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m,\n\u001b[1;32m     42\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     \u001b[39m# Extract generated keywords\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     keywords \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/matt/projects/ai-lit-search/venv/lib/python3.8/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Documents/matt/projects/ai-lit-search/venv/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Documents/matt/projects/ai-lit-search/venv/lib/python3.8/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Documents/matt/projects/ai-lit-search/venv/lib/python3.8/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    629\u001b[0m         ),\n\u001b[1;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/matt/projects/ai-lit-search/venv/lib/python3.8/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: <empty message>"
     ]
    }
   ],
   "source": [
    "### Number of papers to test\n",
    "num_papers = 10\n",
    "### Minimum number of times keyword must appear to be included\n",
    "kw_min = 10\n",
    "\n",
    "ml_stats, ml_usage = _run_test_2(ml_papers, num_papers=num_papers, kw_min=kw_min)\n",
    "microbiome_stats, mb_usage = _run_test_2(microbiome_papers, num_papers=num_papers, kw_min=kw_min)\n",
    "marijuana_stats, mj_usage = _run_test_2(marijuana_papers, num_papers=num_papers, kw_min=kw_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ml_stats.json', 'w') as f:\n",
    "    json.dump(ml_stats, f, indent=4)\n",
    "\n",
    "with open('microbiome_stats.json', 'w') as f:\n",
    "    json.dump(microbiome_stats, f, indent=4)\n",
    "\n",
    "with open('marijuana_stats.json', 'w') as f:\n",
    "    json.dump(marijuana_stats, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers evaluated: 25 per type\n",
      "\n",
      "Average keywords added by GPT-3.5 Turbo for ML papers: 1.04\n",
      "Average keywords added by GPT-3.5 Turbo for Microbiome papers: 1.12\n",
      "Average keywords added by GPT-3.5 Turbo for Marijuana papers: 1.48\n",
      "\n",
      "Average keywords added by GPT-4 for ML papers: 6.32\n",
      "Average keywords added by GPT-4 for Microbiome papers: 7.88\n",
      "Average keywords added by GPT-4 for Marijuana papers: 9.48\n",
      "\n",
      "Average difference between GPT-4 and GPT-3.5 Turbo for ML papers: 5.28\n",
      "Average difference between GPT-4 and GPT-3.5 Turbo for Microbiome papers: 6.76\n",
      "Average difference between GPT-4 and GPT-3.5 Turbo for Marijuana papers: 8.0\n",
      "Average difference between GPT-4 and GPT-3.5 Turbo for all papers: 6.68\n"
     ]
    }
   ],
   "source": [
    "def get_avg_diff(papers_stats):\n",
    "    diffs = []\n",
    "    for paper_idx, paper in papers_stats.items():\n",
    "        diffs.append(paper['gpt-4_n_keywords'] - paper['gpt-3.5_n_keywords'])\n",
    "    return np.mean(diffs)\n",
    "\n",
    "def get_avg_diff_all(list_papers_stats):\n",
    "    diffs = []\n",
    "    for papers_stats in list_papers_stats:\n",
    "        for paper_idx, paper in papers_stats.items():\n",
    "            diffs.append(paper['gpt-4_n_keywords'] - paper['gpt-3.5_n_keywords'])\n",
    "    return np.mean(diffs)\n",
    "\n",
    "def get_mean_of_dict_key(d, k):\n",
    "    return np.mean([v[k] for v in d.values()])\n",
    "\n",
    "ml_avg_diff = get_avg_diff(ml_stats)\n",
    "microbiome_avg_diff = get_avg_diff(microbiome_stats)\n",
    "marijuana_avg_diff = get_avg_diff(marijuana_stats)\n",
    "all_avg_diff = get_avg_diff_all([ml_stats, microbiome_stats, marijuana_stats])\n",
    "\n",
    "print(f'Papers evaluated: {len(ml_stats)} per type')\n",
    "print()\n",
    "print(f'Average keywords added by GPT-3.5 Turbo for ML papers: {get_mean_of_dict_key(ml_stats, \"gpt-3.5_n_keywords\")}')\n",
    "print(f'Average keywords added by GPT-3.5 Turbo for Microbiome papers: {get_mean_of_dict_key(microbiome_stats, \"gpt-3.5_n_keywords\")}')\n",
    "print(f'Average keywords added by GPT-3.5 Turbo for Marijuana papers: {get_mean_of_dict_key(marijuana_stats, \"gpt-3.5_n_keywords\")}')\n",
    "print()\n",
    "print(f'Average keywords added by GPT-4 for ML papers: {get_mean_of_dict_key(ml_stats, \"gpt-4_n_keywords\")}')\n",
    "print(f'Average keywords added by GPT-4 for Microbiome papers: {get_mean_of_dict_key(microbiome_stats, \"gpt-4_n_keywords\")}')\n",
    "print(f'Average keywords added by GPT-4 for Marijuana papers: {get_mean_of_dict_key(marijuana_stats, \"gpt-4_n_keywords\")}')\n",
    "print()\n",
    "print(f'Average difference between GPT-4 and GPT-3.5 Turbo for ML papers: {ml_avg_diff}')\n",
    "print(f'Average difference between GPT-4 and GPT-3.5 Turbo for Microbiome papers: {microbiome_avg_diff}')\n",
    "print(f'Average difference between GPT-4 and GPT-3.5 Turbo for Marijuana papers: {marijuana_avg_diff}')\n",
    "print(f'Average difference between GPT-4 and GPT-3.5 Turbo for all papers: {all_avg_diff}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper title: Deep Learning Model Based on Dual-Modal Ultrasound and Molecular Data for Predicting Response to Neoadjuvant Chemotherapy in Breast Cancer.\n",
      "Paper abstract: To carry out radiomics analysis/deep convolutional neural network (CNN) based on B-mode ultrasound (BUS) and shear wave elastography (SWE) to predict response to neoadjuvant chemotherapy (NAC) in breast cancer patients.\n",
      "In this prospective study, 255 breast cancer patients who received NAC between September 2016 and December 2021 were included. Radiomics models were designed using a support vector machine classifier based on US images obtained before treatment, including BUS and SWE. And CNN models also were developed using ResNet architecture. The final predictive model was developed by combining the dual-modal US and independently associated clinicopathologic characteristics. The predictive performances of the models were assessed with five-fold cross-validation.\n",
      "Pretreatment SWE performed better than BUS in predicting the response to NAC for breast cancer for both the CNN and radiomics models (P < 0.001). The predictive results of the CNN models were significantly better than the radiomics models, with AUCs of 0.72 versus 0.69 for BUS and 0.80 versus 0.77 for SWE, respectively (P = 0.003). The CNN model based on the dual-modal US and molecular data exhibited outstanding performance in predicting NAC response, with an accuracy of 83.60% ± 2.63%, a sensitivity of 87.76% ± 6.44%, and a specificity of 77.45% ± 4.38%.\n",
      "The pretreatment CNN model based on the dual-modal US and molecular data achieved excellent performance for predicting the response to chemotherapy in breast cancer. Therefore, this model has the potential to serve as a non-invasive objective biomarker to predict NAC response and aid clinicians with individual treatments.\n",
      "Paper keywords: 4\n",
      "\n",
      "8\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def _debug_keywords(paper):\n",
    "\n",
    "    print(f'Paper title: {paper[\"title\"]}')\n",
    "    print(f'Paper abstract: {paper[\"abstract\"]}')\n",
    "    print(f'Paper keywords: {paper[\"human_keywords\"]}')\n",
    "    print()\n",
    "    \n",
    "\n",
    "_debug_keywords(ml_stats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
